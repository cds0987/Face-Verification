{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QF9w_c2X5xB_UxF0dMl6Z55HmnbBa_mG",
      "authorship_tag": "ABX9TyMT045KcPWbefKEBDjLrgDO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cds0987/Face-Verification/blob/main/Face_verification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MQcpqMaJeHcW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def build_siamese_model(input_shape):\n",
        "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape,\n",
        "                                                   include_top=False,\n",
        "                                                   weights='imagenet')\n",
        "    base_model.trainable = False\n",
        "\n",
        "    input1 = layers.Input(shape=input_shape)\n",
        "    input2 = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Shared CNN base\n",
        "    encoded1 = base_model(input1)\n",
        "    encoded2 = base_model(input2)\n",
        "\n",
        "    # L1 distance layer\n",
        "    l1_distance = lambda x: tf.keras.backend.abs(x[0] - x[1])\n",
        "    l1_layer = layers.Lambda(l1_distance)\n",
        "    l1_distance_layer = l1_layer([encoded1, encoded2])\n",
        "\n",
        "    # Flatten the output before the last Dense layer\n",
        "    flatten_layer = layers.Flatten()(l1_distance_layer)\n",
        "\n",
        "    # Fully connected layer\n",
        "    fc1 = layers.Dense(128, activation='relu')(flatten_layer)\n",
        "    output = layers.Dense(1, activation='sigmoid')(fc1)\n",
        "\n",
        "    siamese_model = models.Model(inputs=[input1, input2], outputs=output)\n",
        "    siamese_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return siamese_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import combinations\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
      ],
      "metadata": {
        "id": "7nqbDSQi9EAS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image, target_size=(224, 224)):\n",
        "    if isinstance(image, str):  # If 'image' is a file path\n",
        "        img = Image.open(image)\n",
        "    else:  # If 'image' is already a NumPy array\n",
        "        img = Image.fromarray((image * 255).astype('uint8'))\n",
        "\n",
        "    # Resize the image\n",
        "    img = img.resize(target_size)\n",
        "\n",
        "    # Convert the image to a NumPy array\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Expand the dimensions to create a batch (MobileNetV2 expects batches)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Preprocess the input image using MobileNetV2's preprocess_input function\n",
        "    preprocessed_img_array = preprocess_input(img_array)\n",
        "\n",
        "    return preprocessed_img_array[0]  # Remove the batch dimension\n"
      ],
      "metadata": {
        "id": "z8r3rVKy9YUW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pairs_and_labels(directory_path):\n",
        "    classes = [d for d in os.listdir(directory_path) if os.path.isdir(os.path.join(directory_path, d))]\n",
        "    class_indices = {c: i for i, c in enumerate(classes)}\n",
        "\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_path = os.path.join(directory_path, class_name)\n",
        "        images = [f for f in os.listdir(class_path) if f.endswith('.jpg')]\n",
        "\n",
        "        # Generate positive pairs\n",
        "        for i in range(len(images) - 1):\n",
        "            for j in range(i + 1, len(images)):\n",
        "                pairs.append(\n",
        "                    (\n",
        "                        os.path.join(class_path, images[i]),\n",
        "                        os.path.join(class_path, images[j])\n",
        "                    )\n",
        "                )\n",
        "                labels.append(1)  # 1 indicates a positive pair\n",
        "\n",
        "        # Generate negative pairs\n",
        "        other_classes = [c for c in classes if c != class_name]\n",
        "        for i in range(len(images)):\n",
        "            for _ in range(5):  # Adjust the number of negative pairs as needed\n",
        "                other_class = random.choice(other_classes)\n",
        "                other_class_path = os.path.join(directory_path, other_class)\n",
        "                other_images = [f for f in os.listdir(other_class_path) if f.endswith('.jpg')]\n",
        "\n",
        "                pairs.append(\n",
        "                    (\n",
        "                        os.path.join(class_path, images[i]),\n",
        "                        os.path.join(other_class_path, random.choice(other_images))\n",
        "                    )\n",
        "                )\n",
        "                labels.append(0)  # 0 indicates a negative pair\n",
        "\n",
        "    return pairs, labels\n",
        "\n",
        "# Directory containing the images\n",
        "directory_path = '/content/drive/MyDrive/lfw_funneled'\n",
        "\n",
        "# Generate pairs and labels\n",
        "pairs, labels = generate_pairs_and_labels(directory_path)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_pairs, test_pairs, train_labels, test_labels = train_test_split(pairs, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "rh-5tvr88-O-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_pairs))\n",
        "print(len(train_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ZwQ55o9bFy",
        "outputId": "0f8201f8-1576-4f4a-b853-9c58c1a13946"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "246737\n",
            "246737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (250, 250, 3)\n"
      ],
      "metadata": {
        "id": "Lh0zwuot9ltz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path):\n",
        "    img = load_img(image_path)\n",
        "    img_array = img_to_array(img)\n",
        "    return img_array\n"
      ],
      "metadata": {
        "id": "8pqKXvrT-x0l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create arrays for training pairs with the specified limit\n",
        "max_training_pairs = 500  # Adjust as needed\n",
        "\n",
        "# Load and create arrays for training pairs without preprocessing\n",
        "train_pairs_left = np.array([load_image(pair[0]) for pair in train_pairs[:max_training_pairs]])\n",
        "train_pairs_right = np.array([load_image(pair[1]) for pair in train_pairs[:max_training_pairs]])\n",
        "train_labels = np.array(train_labels[:max_training_pairs])\n",
        "\n"
      ],
      "metadata": {
        "id": "-iB_KZjx-zKl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_pairs_left.shape)\n",
        "print(train_pairs_right.shape)\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpNtedZeMACC",
        "outputId": "5576ed66-1dbe-47ea-efde-df20edf15a47"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 250, 250, 3)\n",
            "(500, 250, 250, 3)\n",
            "(500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_testing_pairs = 100\n",
        "test_pairs_left = np.array([load_image(pair[0]) for pair in test_pairs[:max_testing_pairs]])\n",
        "test_pairs_right = np.array([load_image(pair[1]) for pair in test_pairs[:max_testing_pairs]])\n",
        "test_labels = np.array(test_labels[:max_testing_pairs])"
      ],
      "metadata": {
        "id": "ie42jCGFIneq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_pairs_left.shape)\n",
        "print(test_pairs_right.shape)\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j0RSeLFN9_b",
        "outputId": "0d41d875-fd23-48ad-fcdd-dc360f8ca6ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 250, 250, 3)\n",
            "(100, 250, 250, 3)\n",
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels[:6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD7OaDzGKHfj",
        "outputId": "f5404612-8199-4a13-d772-ace781bc3f37"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = build_siamese_model(input_shape)\n",
        "\n",
        "siamese_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-Xo5uf6KTSs",
        "outputId": "caaf0bd1-003b-42c2-a5a6-771b9e08c9e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 250, 250, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 250, 250, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " mobilenetv2_1.00_224 (Func  (None, 8, 8, 1280)           2257984   ['input_2[0][0]',             \n",
            " tional)                                                             'input_3[0][0]']             \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 8, 8, 1280)           0         ['mobilenetv2_1.00_224[0][0]',\n",
            "                                                                     'mobilenetv2_1.00_224[1][0]']\n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 81920)                0         ['lambda[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  1048588   ['flatten[0][0]']             \n",
            "                                                          8                                       \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1)                    129       ['dense[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 12744001 (48.61 MB)\n",
            "Trainable params: 10486017 (40.00 MB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint('best_siamese_model_weights.h5',\n",
        "                              save_best_only=True,\n",
        "                              save_weights_only=True,\n",
        "                              monitor='val_loss',  # Monitor validation loss\n",
        "                              mode='min',  # Save the weights when validation loss is minimized\n",
        "                              verbose=1)\n",
        "\n",
        "\n",
        "# Train the Siamese model with the ModelCheckpoint callback\n",
        "siamese_model.fit([train_pairs_left, train_pairs_right], train_labels,\n",
        "                  epochs=10,\n",
        "                  batch_size=32,\n",
        "                  validation_data=([test_pairs_left, test_pairs_right], test_labels),\n",
        "                  callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5uRXg0389D_",
        "outputId": "18db67fc-6f74-4d6c-dc57-53afc281f70d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 7.3123 - accuracy: 0.7040\n",
            "Epoch 1: val_loss improved from inf to 4.72806, saving model to best_siamese_model_weights.h5\n",
            "16/16 [==============================] - 93s 5s/step - loss: 7.3123 - accuracy: 0.7040 - val_loss: 4.7281 - val_accuracy: 0.7400\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.2110 - accuracy: 0.8720\n",
            "Epoch 2: val_loss improved from 4.72806 to 1.62417, saving model to best_siamese_model_weights.h5\n",
            "16/16 [==============================] - 84s 5s/step - loss: 1.2110 - accuracy: 0.8720 - val_loss: 1.6242 - val_accuracy: 0.7700\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.3861 - accuracy: 0.9400\n",
            "Epoch 3: val_loss improved from 1.62417 to 1.29498, saving model to best_siamese_model_weights.h5\n",
            "16/16 [==============================] - 96s 6s/step - loss: 0.3861 - accuracy: 0.9400 - val_loss: 1.2950 - val_accuracy: 0.8100\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9760\n",
            "Epoch 4: val_loss improved from 1.29498 to 1.10506, saving model to best_siamese_model_weights.h5\n",
            "16/16 [==============================] - 81s 5s/step - loss: 0.0922 - accuracy: 0.9760 - val_loss: 1.1051 - val_accuracy: 0.8000\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9940\n",
            "Epoch 5: val_loss did not improve from 1.10506\n",
            "16/16 [==============================] - 80s 5s/step - loss: 0.0149 - accuracy: 0.9940 - val_loss: 1.3477 - val_accuracy: 0.7900\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9980\n",
            "Epoch 6: val_loss did not improve from 1.10506\n",
            "16/16 [==============================] - 80s 5s/step - loss: 0.0020 - accuracy: 0.9980 - val_loss: 1.1528 - val_accuracy: 0.8100\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.4929e-04 - accuracy: 1.0000\n",
            "Epoch 7: val_loss did not improve from 1.10506\n",
            "16/16 [==============================] - 68s 4s/step - loss: 2.4929e-04 - accuracy: 1.0000 - val_loss: 1.2549 - val_accuracy: 0.7800\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.7934e-04 - accuracy: 1.0000\n",
            "Epoch 8: val_loss did not improve from 1.10506\n",
            "16/16 [==============================] - 79s 5s/step - loss: 1.7934e-04 - accuracy: 1.0000 - val_loss: 1.2517 - val_accuracy: 0.7800\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.3503e-04 - accuracy: 1.0000\n",
            "Epoch 9: val_loss did not improve from 1.10506\n",
            "16/16 [==============================] - 69s 4s/step - loss: 1.3503e-04 - accuracy: 1.0000 - val_loss: 1.2246 - val_accuracy: 0.7800\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - ETA: 0s - loss: 1.1004e-04 - accuracy: 1.0000\n",
            "Epoch 10: val_loss did not improve from 1.10506\n",
            "16/16 [==============================] - 72s 4s/step - loss: 1.1004e-04 - accuracy: 1.0000 - val_loss: 1.2238 - val_accuracy: 0.7800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c9cd516a860>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "siamese_model = build_siamese_model(input_shape)\n",
        "\n",
        "# Load the best weights saved during training\n",
        "siamese_model.load_weights('/content/best_siamese_model_weights.h5')\n",
        "\n",
        "# Now, you can use the loaded model for predictions or evaluation\n",
        "# Example: Make predictions on test data\n",
        "predictions = siamese_model.predict([test_pairs_left, test_pairs_right])\n",
        "\n",
        "# Evaluate the model on test data\n",
        "evaluation = siamese_model.evaluate([test_pairs_left, test_pairs_right], test_labels)\n",
        "\n",
        "print(\"Test Accuracy:\", evaluation[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj9x5AlTSAX4",
        "outputId": "a4d8a4da-081d-4306-a5d2-02a7cf6ef4bc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 14s 2s/step\n",
            "4/4 [==============================] - 14s 3s/step - loss: 1.1051 - accuracy: 0.8000\n",
            "Test Accuracy: 0.800000011920929\n"
          ]
        }
      ]
    }
  ]
}